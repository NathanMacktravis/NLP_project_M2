{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import pprint\n",
    "import requests\n",
    "API_KEY = \"AIzaSyD8-1UI7KZJOR-rps7fYwHUAuCl9dvb1yg\"\n",
    "youtube = build('youtube', 'v3', developerKey=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_transcripts(transcripts_dict):\n",
    "    concatenated_text = \"\"\n",
    "    transcript_list_concatenated = []\n",
    "\n",
    "    for video_id, transcript_list in transcripts_dict.items():\n",
    "        for entry in transcript_list:\n",
    "            concatenated_text += entry.get('text', '') + ' '\n",
    "        if len(transcripts_dict) == 1:\n",
    "            return concatenated_text\n",
    "        else:\n",
    "            transcript_list_concatenated.append(concatenated_text)\n",
    "    return transcript_list_concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "def video_transcript(video_id, languages=['en']): \n",
    "    try:\n",
    "        transcript_list = YouTubeTranscriptApi.get_transcripts(video_id, languages=['en'])\n",
    "        return concatenate_transcripts(transcript_list[0])\n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_videos(channel_id, max_results=10): # you can get more videos than 10\n",
    "\n",
    "    # Get the uploads playlist ID\n",
    "    channel_response = youtube.channels().list(\n",
    "    part='contentDetails',\n",
    "    id=channel_id\n",
    "    ).execute()\n",
    "\n",
    "    uploads_playlist_id = channel_response['items'][0]['contentDetails']['relatedPlaylists']['uploads']\n",
    "    # Fetch videos from the playlist\n",
    "    playlist_request = youtube.playlistItems().list(\n",
    "    part=\"snippet\",\n",
    "    playlistId=uploads_playlist_id,\n",
    "    maxResults=max_results\n",
    "    )\n",
    "    try:\n",
    "        playlist_response = playlist_request.execute()\n",
    "        videos = []\n",
    "        for item in playlist_response['items']:\n",
    "            video_id = item['snippet']['resourceId']['videoId']\n",
    "            videos.append({\n",
    "            'videoId': video_id,\n",
    "            'title': item['snippet']['title'],\n",
    "            'description': item['snippet']['description'],\n",
    "            'publishedAt': item['snippet']['publishedAt'], \n",
    "            'video_transcript': video_transcript([video_id])\n",
    "            })\n",
    "        return videos\n",
    "    except:\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def search_channels(query, max_results=50, save_json=True, write_mode='a'):\n",
    "    next_page_token = None\n",
    "    channels = []\n",
    "    while len(channels) < max_results:\n",
    "        request = youtube.search().list(\n",
    "            part=\"snippet\",\n",
    "            type=\"channel\",\n",
    "            q=query,\n",
    "            maxResults=min(50, max_results - len(channels)),\n",
    "            relevanceLanguage='en',\n",
    "            regionCode='US',\n",
    "            pageToken=next_page_token\n",
    "        )\n",
    "        response = request.execute()\n",
    "        if save_json:\n",
    "            try:\n",
    "                with open(\"yt_dataset.json\", write_mode, encoding='utf-8') as json_file:\n",
    "                    for item in tqdm(response.get('items', [])):\n",
    "                        channel_id = item['snippet']['channelId']\n",
    "                        channel = {\n",
    "                            'channelId': channel_id,\n",
    "                            'channelTitle': item['snippet']['channelTitle'],\n",
    "                            'description': item['snippet']['description'],\n",
    "                            'latestVideos': get_latest_videos(channel_id)\n",
    "                        }\n",
    "                        json.dump(channel, json_file, ensure_ascii=False, indent=4)\n",
    "                        json_file.write('\\n')\n",
    "                print(f\"Data written to yt_dataset.json\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error writing data to the file: {e}\")\n",
    "        else:\n",
    "            for item in response.get('items', []):\n",
    "                channel_id = item['snippet']['channelId']\n",
    "                # feel free to check and mess around with the data provided in each \"item\"\n",
    "                channels.append({\n",
    "                'channelId': channel_id,\n",
    "                'channelTitle': item['snippet']['channelTitle'],\n",
    "                'description': item['snippet']['description'],\n",
    "                'latestVideos': get_latest_videos(channel_id)\n",
    "                })\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "    if save_json:\n",
    "        return channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [05:14<00:00,  6.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to yt_dataset.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [04:52<00:00,  5.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to yt_dataset.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [04:52<00:00,  5.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to yt_dataset.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [00:29<04:34,  6.11s/it]"
     ]
    }
   ],
   "source": [
    "search_channels(\"Anime\", max_results=500, write_mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "request = youtube.search().list(\n",
    "    part=\"snippet\",\n",
    "    type=\"channel\",\n",
    "    q=\"anime\",\n",
    "    maxResults=500,\n",
    "    relevanceLanguage='en',\n",
    "    regionCode='US'\n",
    "    )\n",
    "response = request.execute()\n",
    "print(len(response.get('items', [])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "with open(\"yt_dataset.json\", \"r\", encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(len(data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
